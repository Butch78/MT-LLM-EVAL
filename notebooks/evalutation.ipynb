{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4615384615384615\n",
      "0.0\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "reference_summary = \"Apple is a Red fruit\"\n",
    "candidate_summary = \"Question: What colour is an Apple?, Answer: Red\"\n",
    "\n",
    "scores = scorer.score(reference_summary, candidate_summary)\n",
    "\n",
    "print(scores['rouge1'].fmeasure)  # F1-score for ROUGE-1\n",
    "print(scores['rouge2'].precision)  # Precision for ROUGE-2\n",
    "print(scores['rougeL'].recall)     # Recall for ROUGE-L\n",
    "\n",
    "# Use an expert as the source of truth e.g Flashcards for Languages/Courses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09703504043126684\n",
      "0.8421052631578947\n",
      "0.05128205128205128\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "reference_summary = \"\"\"a Postgres database. The application frequently retrieves, modifies, and inserts\n",
    "large amounts of data. All this communication is done by Java classes that send\n",
    "(complex) SQL queries to the database.\n",
    "As testers, we know that a bug can be anywhere, including in the SQL que-\n",
    "ries. We also know that there are many ways to exercise our system. Which one\n",
    "of the following is not a good option to detect bugs in SQL queries?29 Summary\n",
    "AUnit testing\n",
    "BIntegration testing\n",
    "CSystem testing\n",
    "DStress testing\n",
    "1.8Choosing the level of a test involves a trade-off, because each test level has\n",
    "advantages and disadvantages. Which one of the following is the main advan-\n",
    "tage of a test at the system level?\n",
    "AThe interaction with the system is much closer to reality.\n",
    "BIn a continuous integration environment, system tests provide real feed-\n",
    "back to developers.\n",
    "CBecause system tests are never flaky, they provide developers with more\n",
    "stable feedback.\n",
    "DA system test is written by product owners, making it closer to reality.\n",
    "1.9What is the main reason the number of recommended system tests in the test-\n",
    "ing pyramid is smaller than the number of unit tests?\n",
    "AUnit tests are as good as system tests.\n",
    "BSystem tests tend to be slow and are difficult to make deterministic.\n",
    "CThere are no good tools for system tests.\n",
    "DSystem tests do not provide developers with enough quality feedback. \n",
    "Summary\n",
    "Testing and test code can guide you through software development. But soft-\n",
    "ware testing is about finding bugs, and that is what this book is primarily about.\n",
    "Systematic and effective software testing helps you design test cases that exer-\n",
    "cise all the corners of your code and (hopefully) leaves no space for unex-\n",
    "pected behavior.\n",
    "Although being systematic helps, you can never be certain that a program does\n",
    "not have bugs.\n",
    "Exhaustive testing is impossible. The life of a tester involves making trade-offs\n",
    "about how much testing is needed.\n",
    "You can test programs on different levels, ranging from testing small methods\n",
    "to testing entire systems with databases and web services. Each level has advan-\n",
    "tages and disadvantages.\"\"\"\n",
    "candidate_summary = \"\"\"{\\n  \"Question\": \"Which one of the following is not a good option to detect bugs in SQL queries?\",\\n  \"Answer\": \"DStress testing\"\\n}\\n\\n  \\n  \"\"\"\n",
    "\n",
    "scores = scorer.score(reference_summary, candidate_summary)\n",
    "\n",
    "print(scores['rouge1'].fmeasure)  # F1-score for ROUGE-1\n",
    "print(scores['rouge2'].precision)  # Precision for ROUGE-2\n",
    "print(scores['rougeL'].recall)     # Recall for ROUGE-L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/MT-LLM-EVAL/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 29.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.63 seconds, 1.60 sentences/sec\n",
      "Precision: 0.824378\n",
      "Recall: 0.872494\n",
      "F1 Score: 0.847754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Reference and candidate flashcards (question-answer pairs) as lists\n",
    "reference_flashcards = [\"\"\"Apple is a Red fruit\"\"\"]\n",
    "candidate_flashcards = [\"\"\"Question: What colour is an Apple?, Answer: Red\"\"\"]\n",
    "\n",
    "# Compute BERTScore\n",
    "P, R, F1 = score(candidate_flashcards, reference_flashcards, lang='en', verbose=True)\n",
    "\n",
    "# Print BERTScore results\n",
    "print(f\"Precision: {P.mean().item():.6f}\")\n",
    "print(f\"Recall: {R.mean().item():.6f}\")\n",
    "print(f\"F1 Score: {F1.mean().item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 128.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.91 seconds, 0.34 sentences/sec\n",
      "Precision: 0.841691\n",
      "Recall: 0.777896\n",
      "F1 Score: 0.808537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Reference and candidate flashcards (question-answer pairs) as lists\n",
    "reference_flashcards = [\"\"\"a Postgres database. The application frequently retrieves, modifies, and inserts\n",
    "large amounts of data. All this communication is done by Java classes that send\n",
    "(complex) SQL queries to the database.\n",
    "As testers, we know that a bug can be anywhere, including in the SQL que-\n",
    "ries. We also know that there are many ways to exercise our system. Which one\n",
    "of the following is not a good option to detect bugs in SQL queries?29 Summary\n",
    "AUnit testing\n",
    "BIntegration testing\n",
    "CSystem testing\n",
    "DStress testing\n",
    "1.8Choosing the level of a test involves a trade-off, because each test level has\n",
    "advantages and disadvantages. Which one of the following is the main advan-\n",
    "tage of a test at the system level?\n",
    "AThe interaction with the system is much closer to reality.\n",
    "BIn a continuous integration environment, system tests provide real feed-\n",
    "back to developers.\n",
    "CBecause system tests are never flaky, they provide developers with more\n",
    "stable feedback.\n",
    "DA system test is written by product owners, making it closer to reality.\n",
    "1.9What is the main reason the number of recommended system tests in the test-\n",
    "ing pyramid is smaller than the number of unit tests?\n",
    "AUnit tests are as good as system tests.\n",
    "BSystem tests tend to be slow and are difficult to make deterministic.\n",
    "CThere are no good tools for system tests.\n",
    "DSystem tests do not provide developers with enough quality feedback. \n",
    "Summary\n",
    "Testing and test code can guide you through software development. But soft-\n",
    "ware testing is about finding bugs, and that is what this book is primarily about.\n",
    "Systematic and effective software testing helps you design test cases that exer-\n",
    "cise all the corners of your code and (hopefully) leaves no space for unex-\n",
    "pected behavior.\n",
    "Although being systematic helps, you can never be certain that a program does\n",
    "not have bugs.\n",
    "Exhaustive testing is impossible. The life of a tester involves making trade-offs\n",
    "about how much testing is needed.\n",
    "You can test programs on different levels, ranging from testing small methods\n",
    "to testing entire systems with databases and web services. Each level has advan-\n",
    "tages and disadvantages.\"\"\"]\n",
    "candidate_flashcards = [\"\"\"{\\n  \"Question\": \"Which one of the following is not a good option to detect bugs in SQL queries?\",\\n  \"Answer\": \"DStress testing\"\\n}\\n\\n  \\n  \"\"\"]\n",
    "\n",
    "# Compute BERTScore\n",
    "P, R, F1 = score(candidate_flashcards, reference_flashcards, lang='en', verbose=True)\n",
    "\n",
    "# Print BERTScore results\n",
    "print(f\"Precision: {P.mean().item():.6f}\")\n",
    "print(f\"Recall: {R.mean().item():.6f}\")\n",
    "print(f\"F1 Score: {F1.mean().item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat sits outside \t\t The dog plays in the garden \t\t Score: 0.2838\n",
      "A man is playing guitar \t\t A woman watches TV \t\t Score: -0.0327\n",
      "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.8939\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = ['The cat sits outside',\n",
    "             'A man is playing guitar',\n",
    "             'The new movie is awesome']\n",
    "\n",
    "sentences2 = ['The dog plays in the garden',\n",
    "              'A woman watches TV',\n",
    "              'The new movie is so great']\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bart_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# To use the CNNDM version BARTScore\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbart_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BARTScorer\n\u001b[1;32m      3\u001b[0m bart_scorer \u001b[38;5;241m=\u001b[39m BARTScorer(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m, checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacebook/bart-large-cnn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m bart_scorer\u001b[38;5;241m.\u001b[39mscore([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis is interesting.\u001b[39m\u001b[38;5;124m'\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis is fun.\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;66;03m# generation scores from the first list of texts to the second list of texts.\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bart_score'"
     ]
    }
   ],
   "source": [
    "# To use the CNNDM version BARTScore\n",
    ">>> from bart_score import BARTScorer\n",
    ">>> bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')\n",
    ">>> bart_scorer.score(['This is interesting.'], ['This is fun.'], batch_size=4) # generation scores from the first list of texts to the second list of texts.\n",
    "[out]\n",
    "[-2.510652780532837]\n",
    "\n",
    "# To use our trained ParaBank version BARTScore\n",
    ">>> from bart_score import BARTScorer\n",
    ">>> bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')\n",
    ">>> bart_scorer.load(path='bart.pth')\n",
    ">>> bart_scorer.score(['This is interesting.'], ['This is fun.'], batch_size=4)\n",
    "[out]\n",
    "[-2.336203098297119]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meteor\n",
    "\n",
    "https://www.nltk.org/howto/meteor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4167"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.translate import meteor\n",
    "from nltk import word_tokenize\n",
    "\n",
    "round(meteor([word_tokenize('The cat sat on the mat')], word_tokenize('on the mat sat the Dog')), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate import bleu_score\n",
    "\n",
    "reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "\n",
    "bleu_score.sentence_bleu(reference, candidate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Based Metrics\n",
    "\n",
    "Semantic Answer Similarity (SAS) is a metric that uses a transformer to measure the similarity between two sentences. It is based on the work of [Sentence-BERT](\n",
    "\n",
    "\n",
    "What's wrong with Existing Metrics?\n",
    "https://www.deepset.ai/blog/semantic-answer-similarity-to-evaluate-qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/MT-LLM-EVAL/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.923076923076923,\n",
       " 'rouge2': 0.7272727272727272,\n",
       " 'rougeL': 0.923076923076923,\n",
       " 'rougeLsum': 0.923076923076923}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge_score = evaluate.load(\"rouge\")\n",
    "\n",
    "generated_summary = \"I absolutely loved reading the Hunger Games\"\n",
    "reference_summary = \"I loved reading the Hunger Games\"\n",
    "\n",
    "scores = rouge_score.compute(\n",
    "    predictions=[generated_summary], references=[reference_summary]\n",
    ")\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
